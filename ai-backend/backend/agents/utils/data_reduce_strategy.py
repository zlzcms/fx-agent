# -*- coding: utf-8 -*-
# æ•°æ®ç¼©å‡ç­–ç•¥æ¨¡å— - åŸºäºLangChainçš„map_reduceå’ŒsummarizeåŠŸèƒ½

import asyncio
import json

from typing import Any, AsyncGenerator, Dict, List, Optional

from langchain_text_splitters import RecursiveCharacterTextSplitter

from backend.agents.config.setting import settings
from backend.agents.tools.data_export_tool import DataExportTool
from backend.agents.utils.token_utils import estimate_tokens
from backend.common.log import logger


class DataReduceStrategy:
    """
    æ•°æ®ç¼©å‡ç­–ç•¥ç±»ï¼Œç”¨äºå¤„ç†å¤§æ•°æ®é‡æ—¶çš„tokené™åˆ¶é—®é¢˜
    """

    def __init__(self, llm, **kwargs):
        """
        åˆå§‹åŒ–æ•°æ®ç¼©å‡ç­–ç•¥

        Args:
            llm: LLMå®ä¾‹
            kwargs: é…ç½®å‚æ•°ï¼ŒåŒ…å«max_tokens, chunk_size, chunk_overlap, max_items_per_chunk
        """
        self.max_tokens = kwargs.get("max_tokens", settings.SPLIT_MAX_TOKEN)
        # å¢åŠ chunk_sizeä»¥å‡å°‘chunksæ•°é‡ï¼Œæé«˜å¤„ç†é€Ÿåº¦
        self.chunk_size = kwargs.get("chunk_size", settings.SPLIT_CHUNK_SIZE)
        self.chunk_overlap = kwargs.get("chunk_overlap", settings.SPLIT_CHUNK_OVERLAP)
        self.max_items_per_chunk = kwargs.get("max_items_per_chunk", settings.SPLIT_MAX_ITEMS_PER_CHUNK)

        self.llm = llm
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap
        )

        self.data_export_tool = DataExportTool("data_reduce_strategy")

    async def _process_single_chunk_async(
        self, chunk_text: str, map_prompt_template: str, chunk_index: int, total_chunks: int
    ) -> str:
        """
        å¤„ç†å•ä¸ªchunkï¼ˆç”¨äºå¼‚æ­¥å¹¶å‘å¤„ç†ï¼‰

        Args:
            chunk_text: chunkæ–‡æœ¬
            map_prompt_template: åˆ†ææç¤ºè¯æ¨¡æ¿
            chunk_index: chunkç´¢å¼•
            total_chunks: æ€»chunkæ•°

        Returns:
            å¤„ç†åçš„ç»“æœ
        """
        try:
            logger.info(f"ğŸ”„ Processing chunk {chunk_index + 1}/{total_chunks}...")

            # æ£€æŸ¥map_prompt_templateæ˜¯å¦æœ‰æ•ˆ
            if not map_prompt_template:
                raise ValueError("map_prompt_template is None or empty")

            # æ£€æŸ¥map_prompt_templateæ˜¯å¦åŒ…å«{text}å ä½ç¬¦
            if "{text}" not in map_prompt_template:
                logger.warning(
                    f"map_prompt_template does not contain {{text}} placeholder: {map_prompt_template[:100]}..."
                )
                # å¦‚æœæ²¡æœ‰{text}å ä½ç¬¦ï¼Œç›´æ¥ä½¿ç”¨æ¨¡æ¿
                prompt = map_prompt_template
            else:
                # æ ¼å¼åŒ–prompt
                prompt = map_prompt_template.format(text=chunk_text)

            # è°ƒç”¨LLMï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œæ€§èƒ½æ›´å¥½ï¼‰
            response = await self.llm.ainvoke(prompt)

            logger.info(f"âœ… Chunk {chunk_index + 1}/{total_chunks} completed")
            return response.content

        except Exception as e:
            logger.error(f"âŒ Chunk {chunk_index + 1}/{total_chunks} failed: {str(e)}")
            logger.error(f"map_prompt_template: {map_prompt_template[:200] if map_prompt_template else 'None'}...")
            logger.error(f"chunk_text length: {len(chunk_text) if chunk_text else 'None'}")
            return f"Chunk {chunk_index + 1} processing failed: {str(e)}"

    def split_structured_data(
        self, text_data: str, map_prompt_template: str = None, header_lines: int = 6, footer_lines: int = 1
    ) -> Dict[str, Any]:
        """
        å°†ç»“æ„åŒ–æ•°æ®æ‹†åˆ†ä¸ºå¤´éƒ¨ã€å°¾éƒ¨ã€æ•°æ®éƒ¨åˆ†

        Args:
            text_data: åŸå§‹æ–‡æœ¬æ•°æ®
            map_prompt_template: Mapé˜¶æ®µçš„æç¤ºè¯æ¨¡æ¿ï¼ˆç”¨äºè®¡ç®—å¯ç”¨chunk_sizeï¼‰
            header_lines: å¤´éƒ¨è¡Œæ•°ï¼Œé»˜è®¤6è¡Œ
            footer_lines: å°¾éƒ¨è¡Œæ•°ï¼Œé»˜è®¤1è¡Œ

        Returns:
            åŒ…å«å¤´éƒ¨ã€å°¾éƒ¨ã€æ•°æ®éƒ¨åˆ†chunksçš„å­—å…¸
        """
        try:
            logger.info("Starting structured data splitting")

            # æŒ‰è¡Œåˆ†å‰²æ•°æ®
            lines = text_data.split("\n")
            total_lines = len(lines)

            logger.info(f"Total lines: {total_lines}")

            # æå–å¤´éƒ¨
            header_lines = min(header_lines, total_lines)
            header = "\n".join(lines[:header_lines])

            # æå–å°¾éƒ¨
            footer_lines = min(footer_lines, total_lines - header_lines)
            footer = "\n".join(lines[-footer_lines:]) if footer_lines > 0 else ""

            # æå–æ•°æ®éƒ¨åˆ†ï¼ˆä¸­é—´éƒ¨åˆ†ï¼‰
            data_start = header_lines
            data_end = total_lines - footer_lines
            data_lines = lines[data_start:data_end] if data_end > data_start else []
            data_content = "\n".join(data_lines)

            logger.info(f"Header lines: {header_lines}, Footer lines: {footer_lines}, Data lines: {len(data_lines)}")

            # å¦‚æœæ²¡æœ‰æ•°æ®éƒ¨åˆ†ï¼Œç›´æ¥è¿”å›
            if not data_content.strip():
                return {"header": header, "footer": footer, "data_chunks": [], "total_chunks": 0}

            # è®¡ç®—å¯ç”¨çš„chunk_size
            available_chunk_size = self.chunk_size
            if map_prompt_template:
                # ä¼°ç®—map_prompt_templateçš„tokenæ•°
                prompt_tokens = estimate_tokens(map_prompt_template)
                # é¢„ç•™ä¸€äº›ç©ºé—´ç»™å“åº”
                reserved_tokens = prompt_tokens + 500  # é¢„ç•™500ä¸ªtokenç»™å“åº”
                available_chunk_size = max(100, self.chunk_size - reserved_tokens)
                logger.info(f"Map prompt tokens: {prompt_tokens}, Available chunk size: {available_chunk_size}")

            # ä½¿ç”¨è°ƒæ•´åçš„chunk_sizeåˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
            data_splitter = RecursiveCharacterTextSplitter(
                chunk_size=available_chunk_size, chunk_overlap=self.chunk_overlap
            )

            # åˆ†å‰²æ•°æ®éƒ¨åˆ†
            data_docs = data_splitter.create_documents([data_content])
            data_chunks_raw = [doc.page_content for doc in data_docs]

            # å°†å¤´éƒ¨å’Œå°¾éƒ¨æ·»åŠ åˆ°æ¯ä¸ªchunkä¸­
            data_chunks = []
            for i, chunk_content in enumerate(data_chunks_raw):
                # æ£€æŸ¥chunkæ˜¯å¦æœ‰å®é™…æ•°æ®å†…å®¹ï¼ˆä¸åªæ˜¯ç©ºç™½æˆ–åˆ†éš”ç¬¦ï¼‰
                chunk_content_clean = chunk_content.strip()
                if not chunk_content_clean or len(chunk_content_clean) < 10:
                    logger.warning(f"Skipping chunk {i} - no meaningful data content")
                    continue

                # ç»„åˆå¤´éƒ¨ + æ•°æ®chunk + å°¾éƒ¨
                full_chunk = f"{header}\n\n{chunk_content}\n\n{footer}".strip()
                data_chunks.append(full_chunk)
                self.data_export_tool.export_to_markdown(full_chunk, task_id=f"chunk_{i}")

            logger.info(f"Split data section into {len(data_chunks)} chunks (each with header and footer)")

            # å¯¼å‡ºæ‹†åˆ†ç»“æœç”¨äºè°ƒè¯•
            # self.data_export_tool.export_to_markdown(header, task_id="split_header")
            # self.data_export_tool.export_to_markdown(footer, task_id="split_footer")
            # self.data_export_tool.export_to_markdown(data_content, task_id="split_data_content")
            # self.data_export_tool.export_to_markdown(map_prompt_template, task_id="map_prompt_template")

            return {
                "header": header,
                "footer": footer,
                "data_chunks": data_chunks,
                "total_chunks": len(data_chunks),
                "chunk_size_used": available_chunk_size,
                "original_chunk_size": self.chunk_size,
            }

        except Exception as e:
            logger.error(f"Structured data splitting failed: {str(e)}")
            import traceback

            logger.error(f"Traceback: {traceback.format_exc()}")
            raise e

    async def reduce_data_async_stream(
        self,
        data: [list, str],
        map_prompt_template: str,
        combine_prompt_template: str,
        **kwargs,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¼‚æ­¥ç”Ÿæˆå™¨ç‰ˆæœ¬çš„æ•°æ®ç¼©å‡æ–¹æ³•ï¼Œç›´æ¥yieldè¿›åº¦äº‹ä»¶
        é¿å…äº†å›è°ƒæ¨¡å¼å’Œé˜Ÿåˆ—è½®è¯¢ï¼Œä»£ç æ›´ç®€æ´ä¼˜é›…
        
        Yields:
            Dict[str, Any]: åŒ…å«è¿›åº¦ä¿¡æ¯çš„äº‹ä»¶å­—å…¸
                - type: "chunk_completed" æˆ– "__final__"
                - chunk_index: chunkç´¢å¼•ï¼ˆä»…chunk_completedï¼‰
                - total_chunks: æ€»chunkæ•°ï¼ˆä»…chunk_completedï¼‰
                - result: chunkå¤„ç†ç»“æœï¼ˆä»…chunk_completedï¼‰
                - content: æœ€ç»ˆç»“æœï¼ˆä»…__final__ï¼‰
        """
        if isinstance(data, list):
            async for event in self.process_data_chunks_async_stream(
                data,
                map_prompt_template,
                combine_prompt_template,
            ):
                yield event
        else:
            async for event in self.reduce_text_data_async_stream(
                data,
                map_prompt_template,
                combine_prompt_template,
                **kwargs,
            ):
                yield event

    async def reduce_text_data_async_stream(
        self,
        text_data: str,
        map_prompt_template: str = None,
        combine_prompt_template: str = None,
        header_lines: int = 6,
        footer_lines: int = 1,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ä½¿ç”¨ç»“æ„åŒ–æ‹†åˆ†ç­–ç•¥ç¼©å‡æ–‡æœ¬æ•°æ®ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ç‰ˆæœ¬ï¼‰
        ç›´æ¥yieldè¿›åº¦äº‹ä»¶ï¼Œé¿å…å›è°ƒæ¨¡å¼

        Yields:
            Dict[str, Any]: è¿›åº¦äº‹ä»¶
        """
        try:
            logger.info("Starting structured text data reduction (async stream)")

            # é¦–å…ˆè¿›è¡Œç»“æ„åŒ–æ‹†åˆ†
            split_result = self.split_structured_data(text_data, map_prompt_template, header_lines, footer_lines)

            header = split_result["header"]
            footer = split_result["footer"]
            data_chunks = split_result["data_chunks"]

            if not data_chunks:
                logger.info("No data chunks to process, returning header and footer")
                final_content = f"{header}\n\n{footer}".strip()
                yield {"type": "__final__", "content": final_content}
                return

            # å®šä¹‰é»˜è®¤çš„map_prompt_template
            if not map_prompt_template:
                map_prompt_template = """
                    è¯·åˆ†æä»¥ä¸‹æ•°æ®ç‰‡æ®µï¼Œæå–å…³é”®ä¿¡æ¯å’Œæ¨¡å¼ï¼š

                    æ•°æ®ç‰‡æ®µï¼š
                    {text}

                    è¯·æä¾›ï¼š
                    1. å…³é”®æ•°æ®ç‚¹
                    2. é‡è¦è¶‹åŠ¿æˆ–æ¨¡å¼
                    3. å¼‚å¸¸å€¼æˆ–å€¼å¾—æ³¨æ„çš„ç‚¹
                    4. ç®€è¦æ€»ç»“
                    """

            if not combine_prompt_template:
                combine_prompt_template = """
                åŸºäºä»¥ä¸‹å„ä¸ªæ•°æ®ç‰‡æ®µçš„åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½ç»¼åˆåˆ†ææŠ¥å‘Šï¼š

                åˆ†æç»“æœï¼š
                {text}

                è¯·æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œæä¾›ï¼š
                1. æ•´ä½“æ•°æ®æ¦‚è§ˆ
                2. ä¸»è¦è¶‹åŠ¿å’Œæ¨¡å¼
                3. å…³é”®å‘ç°
                4. ç»¼åˆç»“è®º
                """

            # å¤„ç†æ•°æ®chunksï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ç‰ˆæœ¬ï¼‰
            async for event in self.process_data_chunks_async_stream(
                data_chunks,
                map_prompt_template,
                combine_prompt_template,
            ):
                yield event

        except Exception as e:
            logger.error(f"Structured text data reduction (async stream) failed: {str(e)}")
            import traceback

            logger.error(f"Traceback: {traceback.format_exc()}")
            raise e

    async def process_data_chunks_async_stream(
        self,
        data_chunks: List[str],
        map_prompt_template: str,
        combine_prompt_template: str = None,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¼‚æ­¥ç”Ÿæˆå™¨ç‰ˆæœ¬çš„å¤„ç†æ•°æ®chunksæ–¹æ³•
        ç›´æ¥yieldè¿›åº¦äº‹ä»¶ï¼Œé¿å…å›è°ƒæ¨¡å¼ï¼Œä»£ç æ›´ç®€æ´ä¼˜é›…

        Args:
            data_chunks: æ•°æ®chunksåˆ—è¡¨
            map_prompt_template: Mapé˜¶æ®µçš„æç¤ºè¯æ¨¡æ¿
            combine_prompt_template: Reduceé˜¶æ®µçš„æç¤ºè¯æ¨¡æ¿

        Yields:
            Dict[str, Any]: åŒ…å«è¿›åº¦ä¿¡æ¯çš„äº‹ä»¶å­—å…¸
                - type: "chunk_completed" æˆ– "__final__"
                - chunk_index: chunkç´¢å¼•ï¼ˆä»…chunk_completedï¼‰
                - total_chunks: æ€»chunkæ•°ï¼ˆä»…chunk_completedï¼‰
                - result: chunkå¤„ç†ç»“æœï¼ˆä»…chunk_completedï¼‰
                - content: æœ€ç»ˆç»“æœï¼ˆä»…__final__ï¼‰
        """
        total_chunks = len(data_chunks)
        map_results = [""] * total_chunks

        if settings.SPLIT_USE_PARALLEL and total_chunks > 1:
            logger.info(f"ğŸš€ Starting ASYNC PARALLEL processing of {total_chunks} data chunks (stream)")

            # åˆ›å»ºä»»åŠ¡å¹¶å»ºç«‹ task -> chunk_index çš„æ˜ å°„
            task_to_index = {}
            for i, chunk_text in enumerate(data_chunks):
                task = asyncio.create_task(self._process_single_chunk_async(chunk_text, map_prompt_template, i, total_chunks))
                task_to_index[task] = i

            # ä½¿ç”¨ asyncio.as_completed æŒ‰å®Œæˆé¡ºåºå¤„ç†ä»»åŠ¡
            async for task in asyncio.as_completed(task_to_index.keys()):
                idx = task_to_index[task]
                try:
                    result = await task
                except Exception as e:
                    logger.error(f"âŒ Data chunk {idx + 1}/{total_chunks} failed: {str(e)}")
                    result = f"Data chunk {idx + 1} processing failed: {str(e)}"
                map_results[idx] = result
                # ç›´æ¥yieldè¿›åº¦äº‹ä»¶ï¼Œè€Œä¸æ˜¯é€šè¿‡å›è°ƒ
                yield {
                    "type": "chunk_completed",
                    "chunk_index": idx + 1,
                    "total_chunks": total_chunks,
                    "result": result,
                }

        else:
            logger.info(f"ğŸ“Š Starting ASYNC SEQUENTIAL processing of {total_chunks} data chunks (stream)")

            for i, chunk_text in enumerate(data_chunks):
                try:
                    result = await self._process_single_chunk_async(chunk_text, map_prompt_template, i, total_chunks)
                except Exception as e:
                    logger.error(f"âŒ Data chunk {i + 1}/{total_chunks} failed: {str(e)}")
                    result = f"Data chunk {i + 1} processing failed: {str(e)}"
                map_results[i] = result
                # ç›´æ¥yieldè¿›åº¦äº‹ä»¶ï¼Œè€Œä¸æ˜¯é€šè¿‡å›è°ƒ
                yield {
                    "type": "chunk_completed",
                    "chunk_index": i + 1,
                    "total_chunks": total_chunks,
                    "result": result,
                }

        # Reduceé˜¶æ®µï¼šåˆå¹¶æ‰€æœ‰ç»“æœ
        logger.info(f"ğŸ”„ Starting REDUCE phase to combine {len(map_results)} results...")
        logger.info("ğŸ“‹ All Map phase tasks completed, proceeding to Reduce phase")

        combined_text = "\n\n".join([f"æ•°æ®ç‰‡æ®µ {i + 1} åˆ†æç»“æœ:\n{result}" for i, result in enumerate(map_results)])

        # ä½¿ç”¨combine promptåˆå¹¶ç»“æœ
        if not combine_prompt_template:
            combine_prompt_template = map_prompt_template
        final_prompt = combine_prompt_template.format(text=combined_text)

        logger.info(f"ğŸ¯ Executing final LLM call for Reduce phase")
        final_response = await self.llm.ainvoke(final_prompt)
        logger.info("âœ… REDUCE phase completed successfully")

        # yieldæœ€ç»ˆç»“æœ
        yield {"type": "__final__", "content": final_response.content}
