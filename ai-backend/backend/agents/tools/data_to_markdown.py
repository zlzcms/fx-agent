# -*- coding: utf-8 -*-
# @Author: zhujinlong
# @Date:   2025-09-03 18:36:24
# @Last Modified by:   zhujinlong
# @Last Modified time: 2025-09-08 16:24:17
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JSONæ•°æ®è½¬Markdownæ ¼å¼æ˜¾ç¤ºå·¥å…·
"""

from datetime import datetime
from typing import Any, Dict, List

# backend_dir = Path(__file__).parent.parent.parent.parent
# sys.path.insert(0, str(backend_dir))
from backend.agents.config.mcp import INTENT_PATTERNS
from backend.agents.config.setting import settings
from backend.agents.utils.token_utils import estimate_tokens
from backend.utils.format_output import extract_json


def format_timestamp(timestamp_str: str) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    try:
        dt = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        return timestamp_str


def format_table_data(columns: List[str], rows: List[List[Any]], is_split: bool = False):
    """æ ¼å¼åŒ–è¡¨æ ¼æ•°æ®ä¸ºMarkdownè¡¨æ ¼"""
    if not columns or not rows:
        return "æ— æ•°æ®"

    # åˆ›å»ºè¡¨å¤´
    header = "| " + " | ".join(columns) + " |"
    separator = "| " + " | ".join(["---"] * len(columns)) + " |"

    # åˆ›å»ºæ•°æ®è¡Œ
    data_rows = []
    current_row = []
    current_rows_count = 0
    for row in rows:
        formatted_row = []
        for cell in row:
            if cell is None:
                formatted_row.append("`null`")
            elif isinstance(cell, str):
                formatted_row.append(f"`{cell}`")
            elif isinstance(cell, (int, float)):
                formatted_row.append(str(cell))
            else:
                formatted_row.append(f"`{str(cell)}`")
        current_row.append("| " + " | ".join(formatted_row) + " |")
        current_rows_count += 1
        current_row_str = "\n".join([header, separator] + current_row)
        # print("current_row_str",estimate_tokens(current_row_str),settings.SPLIT_MAX_TOKEN)
        if is_split and estimate_tokens(current_row_str) > settings.SPLIT_MAX_TOKEN * 0.95:
            data_rows.append({"content": current_row_str, "count": current_rows_count})
            current_row = []
            current_rows_count = 0
    if current_row:
        data_rows.append({"content": "\n".join([header, separator] + current_row), "count": current_rows_count})
    if len(data_rows) == 1:
        return data_rows[0]["content"]
    else:
        return data_rows


def chunk_to_markdown(json_data: Dict[str, Any]) -> str:
    """å°†JSONæ•°æ®è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    markdown_parts = []

    # æ·»åŠ æ ‡é¢˜
    markdown_parts.append("# æ•°æ®æŠ¥å‘Š\n")

    # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
    if "metadata" in json_data:
        metadata = json_data["metadata"]
        markdown_parts.append("## ğŸ“Š å…ƒæ•°æ®ä¿¡æ¯\n")
        markdown_parts.append("| å±æ€§ | å€¼ |")
        markdown_parts.append("|------|-----|")
        markdown_parts.append(f"| åŸå§‹å¤§å° | {metadata.get('original_size', 'N/A')} bytes |")
        markdown_parts.append(f"| åˆ†å—æ•°é‡ | {metadata.get('chunk_count', 'N/A')} |")
        markdown_parts.append(f"| åˆ†å‰²ç­–ç•¥ | {metadata.get('split_strategy', 'N/A')} |")
        markdown_parts.append(f"| å¹³å‡åˆ†å—å¤§å° | {metadata.get('average_chunk_size', 'N/A')} bytes |")
        markdown_parts.append(f"| åˆ†å—ç±»å‹ | {', '.join(metadata.get('chunk_types', []))} |")

        if "processing_timestamp" in metadata:
            markdown_parts.append(f"| å¤„ç†æ—¶é—´ | {format_timestamp(metadata['processing_timestamp'])} |")

        if "config" in metadata:
            config = metadata["config"]
            markdown_parts.append(f"| æœ€å¤§åˆ†å—å¤§å° | {config.get('max_chunk_size', 'N/A')} |")
            markdown_parts.append(f"| æœ€å°åˆ†å—å¤§å° | {config.get('min_chunk_size', 'N/A')} |")
            markdown_parts.append(f"| é‡å å¤§å° | {config.get('overlap_size', 'N/A')} |")

        markdown_parts.append("")

    # æ·»åŠ æ—¶é—´æˆ³
    if "timestamp" in json_data:
        markdown_parts.append(f"**ç”Ÿæˆæ—¶é—´:** {format_timestamp(json_data['timestamp'])}\n")

    # å¤„ç†æ•°æ®å—
    if "chunks" in json_data:
        markdown_parts.append("## ğŸ“‹ æ•°æ®å†…å®¹\n")

        for i, chunk in enumerate(json_data["chunks"]):
            chunk_id = chunk.get("chunk_id", i)
            chunk_type = chunk.get("type", "unknown")
            table_name = chunk.get("table_name", f"table_{i}")
            row_count = chunk.get("row_count", 0)
            column_count = chunk.get("column_count", 0)

            markdown_parts.append(f"### æ•°æ®å— {chunk_id}: {table_name}\n")
            markdown_parts.append(f"**ç±»å‹:** {chunk_type}  \n")
            markdown_parts.append(f"**è¡Œæ•°:** {row_count}  \n")
            markdown_parts.append(f"**åˆ—æ•°:** {column_count}  \n")

            if "data_summary" in chunk:
                markdown_parts.append(f"**æ•°æ®æ‘˜è¦:** {chunk['data_summary']}\n")

            # å¤„ç†è¡¨æ ¼æ•°æ®
            if "content" in chunk and table_name in chunk["content"]:
                table_data = chunk["content"][table_name]
                columns = table_data.get("columns", [])
                rows = table_data.get("rows", [])

                if columns and rows:
                    markdown_parts.append("**æ•°æ®è¡¨æ ¼:**\n")
                    markdown_parts.append(format_table_data(columns, rows))
                    markdown_parts.append("")

            markdown_parts.append("---\n")

    return "\n".join(markdown_parts)


def format_user_timestamp(timestamp: int) -> str:
    """æ ¼å¼åŒ–Unixæ—¶é—´æˆ³"""
    try:
        dt = datetime.fromtimestamp(timestamp)
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except Exception:
        return str(timestamp)


def format_user_data_table(table_name: str, table_data: Dict[str, Any], is_split: bool = False):
    """æ ¼å¼åŒ–ç”¨æˆ·æ•°æ®è¡¨æ ¼"""
    mcp_table_name = INTENT_PATTERNS.get(table_name, {}).get("name", table_name)
    if not table_data or "rows" not in table_data or "columns" not in table_data:
        return f"**{mcp_table_name}:** æ— æ•°æ®\n"
    columns_description = INTENT_PATTERNS.get(table_name, {}).get("fields", {})
    # åˆ›å»ºä¸­è‹±æ–‡ç»“åˆçš„åˆ—åï¼šè‹±æ–‡å­—æ®µå(ä¸­æ–‡æè¿°)
    columns = []
    for column in table_data["columns"]:
        chinese_name = columns_description.get(column, column)
        # print(''======================="",chinese_name)
        if chinese_name != column:
            # å¦‚æœæ‰¾åˆ°äº†ä¸­æ–‡æè¿°ï¼Œæ˜¾ç¤ºä¸ºï¼šè‹±æ–‡å­—æ®µå(ä¸­æ–‡æè¿°)
            columns.append(f"{column}({chinese_name})")
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡æè¿°ï¼Œåªæ˜¾ç¤ºè‹±æ–‡å­—æ®µå
            columns.append(column)
    rows = table_data["rows"]
    count = table_data.get("count", len(rows))

    markdown_parts = []
    markdown_parts_str = ""
    if columns and rows:
        table_data = format_table_data(columns, rows, is_split)
        if isinstance(table_data, list):
            for item in table_data:
                markdown_parts.append(f"**{mcp_table_name}** (å…± {item['count']} æ¡è®°å½•):\n{item['content']}\n")
        else:
            markdown_parts_str += f"**{mcp_table_name}** (å…± {count} æ¡è®°å½•):\n{table_data}\n"
    else:
        markdown_parts_str = f"**{mcp_table_name}**: æ— æ•°æ®\n"
    if markdown_parts:
        return markdown_parts
    else:
        return markdown_parts_str


def get_table_display_order() -> List[str]:
    """è·å–è¡¨çš„æ˜¾ç¤ºé¡ºåº

    è¿”å›:
        List[str]: æŒ‰INTENT_PATTERNSä¸­å®šä¹‰çš„é¡ºåºæ’åˆ—çš„è¡¨ååˆ—è¡¨
    """
    # ç›´æ¥ä½¿ç”¨INTENT_PATTERNSä¸­å®šä¹‰çš„é¡ºåº
    return list(INTENT_PATTERNS.keys())


def users_to_markdown(users_data: Dict[str, Any]) -> str:
    """å°†è¡¨è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    result_markdown = []

    markdown_parts = []

    # è·å–æ‰€æœ‰æŸ¥è¯¢çš„è¡¨åï¼ˆåŒ…æ‹¬æˆåŠŸå’Œå¤±è´¥çš„æŸ¥è¯¢ï¼‰
    all_queried_tables = set()

    # ä»æˆåŠŸæŸ¥è¯¢ä¸­æ”¶é›†è¡¨å
    data_dict = users_data.get("data", {})
    for table_name in data_dict.keys():
        all_queried_tables.add(table_name)

    # ä»å¤±è´¥æŸ¥è¯¢ä¸­æ”¶é›†è¡¨å
    for table_name in users_data.get("failed_queries", {}).keys():
        all_queried_tables.add(table_name)

    # ä»æŸ¥è¯¢æ¡ä»¶ä¸­æ”¶é›†è¡¨åï¼ˆç¡®ä¿æ‰€æœ‰æŸ¥è¯¢çš„è¡¨éƒ½è¢«æ˜¾ç¤ºï¼‰
    query_conditions = users_data.get("query_conditions", {})
    for table_name in query_conditions.keys():
        all_queried_tables.add(table_name)

    # è·å–è¡¨çš„æ˜¾ç¤ºé¡ºåº
    table_order = get_table_display_order()

    # ç”¨äºè®°å½•å·²ç»å¤„ç†è¿‡çš„è¡¨ï¼Œé¿å…é‡å¤
    processed_tables = set()

    # å…ˆæ˜¾ç¤ºæœ‰åºçš„è¡¨
    for table_name in table_order:
        # å¦‚æœè¡¨ä¸åœ¨æŸ¥è¯¢åˆ—è¡¨ä¸­ï¼Œè·³è¿‡
        if table_name not in all_queried_tables:
            continue

        # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªè¡¨
        if table_name in processed_tables:
            continue

        processed_tables.add(table_name)

        table_data = data_dict.get(table_name)
        if table_data and isinstance(table_data, dict) and "columns" in table_data:
            # æœ‰æ•°æ®çš„è¡¨
            user_data_table = format_user_data_table(table_name, table_data, is_split=True)
            if isinstance(user_data_table, list):
                # åˆ—è¡¨ç±»å‹çš„æ•°æ®éœ€è¦åˆ†æ®µå¤„ç†
                for item in user_data_table:
                    # æ£€æŸ¥æ·»åŠ å½“å‰itemåæ˜¯å¦ä¼šè¶…è¿‡tokené™åˆ¶
                    is_split, combined_str = split_markdown_parts(markdown_parts, [item])
                    if is_split:
                        # éœ€è¦åˆ†å‰²ï¼Œå…ˆä¿å­˜ä¹‹å‰çš„ markdown_parts
                        if markdown_parts:
                            markdown_parts_str = "\n".join(markdown_parts)
                            result_markdown.append(markdown_parts_str)
                            markdown_parts = []
                        # å°†å½“å‰itemæ·»åŠ åˆ°æ–°çš„æ®µè½
                        markdown_parts.append(item)
                    else:
                        # ä¸éœ€è¦åˆ†å‰²ï¼Œç›´æ¥æ·»åŠ åˆ°å½“å‰æ®µè½
                        markdown_parts.append(item)
            else:
                # éåˆ—è¡¨ç±»å‹ï¼Œç›´æ¥å¤„ç†
                is_split, combined_str = split_markdown_parts(markdown_parts, [user_data_table])
                if is_split:
                    # éœ€è¦åˆ†å‰²ï¼Œå…ˆä¿å­˜ä¹‹å‰çš„ markdown_parts
                    if markdown_parts:
                        markdown_parts_str = "\n".join(markdown_parts)
                        result_markdown.append(markdown_parts_str)
                        markdown_parts = []
                    # å°†å½“å‰è¡¨æ·»åŠ åˆ°æ–°çš„æ®µè½
                    markdown_parts.append(user_data_table)
                else:
                    markdown_parts.append(user_data_table)
        elif table_name in users_data.get("failed_queries", {}):
            # æŸ¥è¯¢å¤±è´¥çš„è¡¨
            failed_data = users_data.get("failed_queries", {}).get(table_name)
            failed_msg = f"**{INTENT_PATTERNS.get(table_name, {}).get('name', table_name)}**: {failed_data.get('message', 'æŸ¥è¯¢å¤±è´¥')}\n"

            is_split, combined_str = split_markdown_parts(markdown_parts, [failed_msg])
            if is_split:
                # éœ€è¦åˆ†å‰²ï¼Œå…ˆä¿å­˜ä¹‹å‰çš„ markdown_parts
                if markdown_parts:
                    markdown_parts_str = "\n".join(markdown_parts)
                    result_markdown.append(markdown_parts_str)
                    markdown_parts = []
                # å°†å½“å‰å¤±è´¥ä¿¡æ¯æ·»åŠ åˆ°æ–°çš„æ®µè½
                markdown_parts.append(failed_msg)
            else:
                markdown_parts.append(failed_msg)
        else:
            # æŸ¥è¯¢æˆåŠŸä½†æ— æ•°æ®çš„è¡¨ï¼ˆ0æ¡è®°å½•ï¼‰æˆ–è€…æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼ˆå¦‚ç©ºåˆ—è¡¨ï¼‰
            # å¤„ç†æ•°æ®æ ¼å¼ä¸ºåˆ—è¡¨çš„æƒ…å†µ
            if isinstance(table_data, list) or (table_data is None):
                empty_table_data = {"columns": [], "rows": [], "count": 0}
                formatted = format_user_data_table(table_name, empty_table_data)

                is_split, combined_str = split_markdown_parts(markdown_parts, [formatted])
                if is_split:
                    # éœ€è¦åˆ†å‰²ï¼Œå…ˆä¿å­˜ä¹‹å‰çš„ markdown_parts
                    if markdown_parts:
                        markdown_parts_str = "\n".join(markdown_parts)
                        result_markdown.append(markdown_parts_str)
                        markdown_parts = []
                    # å°†å½“å‰ç©ºè¡¨ä¿¡æ¯æ·»åŠ åˆ°æ–°çš„æ®µè½
                    markdown_parts.append(formatted)
                else:
                    markdown_parts.append(formatted)
    if markdown_parts:
        markdown_parts_str = "\n".join(markdown_parts)
        result_markdown.append(markdown_parts_str)
    return result_markdown


def split_markdown_parts(old_markdown_parts: [list, str], new_markdown_parts: [list, str]):
    all_tokens = 0
    all_str = ""
    if old_markdown_parts:
        old_markdown_parts_str = (
            "\n".join(old_markdown_parts) if isinstance(old_markdown_parts, list) else old_markdown_parts
        )
        all_tokens += estimate_tokens(old_markdown_parts_str)
        all_str += f"\n{old_markdown_parts_str}" if all_str else old_markdown_parts_str
    if new_markdown_parts:
        new_markdown_parts_str = (
            "\n".join(new_markdown_parts) if isinstance(new_markdown_parts, list) else new_markdown_parts
        )
        all_tokens += estimate_tokens(new_markdown_parts_str)
        all_str += f"\n{new_markdown_parts_str}" if all_str else new_markdown_parts_str

    # print("all_tokens,SPLIT_MAX_TOKEN",all_tokens,settings.SPLIT_MAX_TOKEN)
    if all_tokens > settings.SPLIT_MAX_TOKEN:
        return True, all_str
    else:
        return False, all_str


def assistant_to_markdown(assistants_data) -> str:
    """å°†AIåŠ©æ‰‹ä¿¡æ¯è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œæ”¯æŒå•ä¸ªå­—å…¸æˆ–å­—å…¸åˆ—è¡¨"""
    markdown_parts = []

    # ç¡®ä¿ assistants_data æ˜¯åˆ—è¡¨æ ¼å¼
    if isinstance(assistants_data, dict):
        assistants_list = [assistants_data]
    elif isinstance(assistants_data, list):
        assistants_list = assistants_data
    else:
        raise ValueError(f"assistants_data must be a dict or list, got {type(assistants_data)}")

    # æ·»åŠ æ ‡é¢˜
    markdown_parts.append("# AIåŠ©æ‰‹ä¿¡æ¯\n")
    markdown_parts.append(f"**ç”Ÿæˆæ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    markdown_parts.append(f"**åŠ©æ‰‹æ€»æ•°:** {len(assistants_list)}\n")

    # å¤„ç†æ¯ä¸ªåŠ©æ‰‹
    for i, assistant in enumerate(assistants_list):
        assistant_id = assistant.get("assistant_id", i)
        name = assistant.get("name", "æœªå‘½ååŠ©æ‰‹")

        markdown_parts.append(f"## {name}\n")

        # æ˜¾å¼å®šä¹‰è¡¨å¤´ä¸å•è¡Œæ•°æ®ï¼Œç¡®ä¿ç»“æ„ä¸ºï¼šç¬¬ä¸€è¡Œè¡¨å¤´ï¼Œç¬¬äºŒè¡Œå¯¹åº”çš„å€¼
        basic_columns = ["åŠ©æ‰‹ID", "åç§°", "æ¨¡å‹", "è¾“å‡ºæ ¼å¼"]
        basic_values = [
            assistant_id,
            name,
            assistant.get("model", {}).get("name", ""),
            assistant.get("output_format", ""),
        ]
        header = "| " + " | ".join(basic_columns) + " |"
        separator = "| " + " | ".join(["---"] * len(basic_columns)) + " |"
        row = "| " + " | ".join([str(v) if v is not None else "" for v in basic_values]) + " |"
        markdown_parts.append("\n".join([header, separator, row]))

        # å¤„ç†æŸ¥è¯¢ç±»å‹
        query_types = assistant.get("query_types", [])
        markdown_parts.append("### ğŸ” æŸ¥è¯¢ç±»å‹\n")
        markdown_parts.append(
            ",".join([INTENT_PATTERNS.get(query_type, {}).get("name", query_type) for query_type in query_types])
        )

        # å¤„ç†è¾“å‡ºæ ¼å¼è¡¨æ ¼
        output_format_table = assistant.get("output_format_table", [])
        if output_format_table:
            markdown_parts.append("### ğŸ“Š è¾“å‡ºæ ¼å¼è¡¨æ ¼\n")
            # print("=========output_format_table===============", output_format_table)
            output_format_table = extract_json(output_format_table)
            if isinstance(output_format_table, list):
                columns = ["å­—æ®µ", "ç±»å‹", "æè¿°"]
                rows = []
                for field in output_format_table:
                    if isinstance(field, dict):
                        rows.append(
                            [
                                field.get("fieldName", ""),
                                field.get("fieldType", ""),
                                field.get("fieldDesc", ""),
                            ]
                        )

                if rows:
                    markdown_parts.append(format_table_data(columns, rows))
                    markdown_parts.append("")

        # å¤„ç†è¾“å‡ºæ ¼å¼æ–‡æ¡£
        output_format_document = assistant.get("output_format_document", "")
        if output_format_document:
            markdown_parts.append("### ğŸ“„ è¾“å‡ºæ ¼å¼æ–‡æ¡£\n")
            markdown_parts.append(f"{output_format_document}\n")
            markdown_parts.append("")

        # å¤„ç†æ¨¡å‹å®šä¹‰
        # model_definition = assistant.get("model_definition", "")
        # if model_definition:
        #     markdown_parts.append("### ğŸ§  æ¨¡å‹å®šä¹‰\n")
        #     markdown_parts.append(f"```\n{model_definition}\n```\n")

        markdown_parts.append("---\n")

    return "\n".join(markdown_parts)


def report_to_markdown(report_data: Dict[str, Any], title: str = "æ•°æ®åˆ†ææŠ¥å‘Š") -> str:
    """å°†æ•°æ®åˆ†ææŠ¥å‘Šè½¬æ¢ä¸ºMarkdownæ ¼å¼

    å‚æ•°:
        report_data: åŒ…å«åˆ†ææŠ¥å‘Šæ•°æ®çš„å­—å…¸ï¼Œç»“æ„å¦‚ä¸‹:
            {
                "analytical_report": str,  # Markdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
                "property_analysis": Dict,  # å±æ€§åˆ†æç»“æœ
                "recommendations": List[str],  # å»ºè®®åˆ—è¡¨
                "confidence": float  # ç½®ä¿¡åº¦
            }
        title: æŠ¥å‘Šæ ‡é¢˜

    è¿”å›:
        str: Markdownæ ¼å¼çš„æŠ¥å‘Š
    """
    markdown_parts = []

    # æ·»åŠ åˆ†ææŠ¥å‘Šä¸»ä½“å†…å®¹
    analytical_report = report_data.get("analytical_report", "")
    if analytical_report:
        # å¦‚æœanalytical_reportå·²ç»æ˜¯markdownæ ¼å¼ï¼Œç›´æ¥æ·»åŠ 
        markdown_parts.append(f"\n{analytical_report}\n")
    else:
        markdown_parts.append("æ²¡æœ‰å¯åˆ†æçš„æŠ¥å‘Šï¼")

    return "\n".join(markdown_parts)


if __name__ == "__main__":
    users_data = {
        "success": True,
        "message": "æŸ¥è¯¢æˆåŠŸ",
        "data": {
            "user_data": {
                "columns": [
                    "id",
                    "nickname",
                    "register_ip",
                    "register_group",
                    "register_area",
                    "email",
                    "d_code",
                    "country",
                    "banned_login",
                    "remark",
                    "login_error_used",
                    "login_error_time",
                    "leverage",
                    "update_time",
                    "mlevel",
                    "userType",
                    "kyc_status",
                    "Iscertified",
                    "commission_value",
                    "create_time",
                ],
                "rows": [
                    [
                        4011,
                        "ooooo yyyy",
                        "240e:382:839:a100:a109:7922:577f:ad7b",
                        "TEST\\TESTSR-USD",
                        "China|China|China",
                        "ouyang001@max.com",
                        "86",
                        "Albania",
                        "0",
                        "",
                        0,
                        0,
                        "",
                        1761962018,
                        1,
                        "direct",
                        9,
                        0,
                        0,
                        1761918270,
                    ]
                ],
                "count": 1,
            },
            "user_mt4_trades": [],
            "user_mt5_trades": [],
            "user_mt5_positions": [],
            "user_amount_log": [],
        },
        "metadata": {
            "query_time": None,
            "count": None,
            "parameters": {
                "user_mt4_trades": {
                    "email": ["ouyang001@max.com"],
                    "range_time": {"data_start_date": "2025-10-01 00:00:00", "data_end_date": "2025-11-01 10:58:27"},
                    "user_id": [4011],
                },
                "user_mt5_trades": {
                    "email": ["ouyang001@max.com"],
                    "range_time": {"data_start_date": "2025-10-01 00:00:00", "data_end_date": "2025-11-01 10:58:27"},
                    "user_id": [4011],
                },
                "user_mt5_positions": {
                    "email": ["ouyang001@max.com"],
                    "range_time": {"data_start_date": "2025-10-01 00:00:00", "data_end_date": "2025-11-01 10:58:27"},
                    "user_id": [4011],
                },
                "user_amount_log": {
                    "email": ["ouyang001@max.com"],
                    "range_time": {"data_start_date": "2025-10-01 00:00:00", "data_end_date": "2025-11-01 10:58:27"},
                    "user_id": [4011],
                },
                "crm_user_id": 1,
            },
            "failed_queries": {},
            "successful_queries": None,
        },
    }
    user_data_list = users_to_markdown(users_data)
    from backend.agents.tools.data_export_tool import DataExportTool

    tool = DataExportTool("data_reduce_strategy")
    ind = 1
    for user_data_item in user_data_list:
        tool.export_to_markdown(user_data_item, "ll", f"ggg{ind}")
        ind += 1
